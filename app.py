import streamlit as st
import google.generativeai as genai
import sqlite3
import uuid
import os
import tempfile
from datetime import datetime
import time

# --- CONFIGURARE PAGINÄ‚ ---
st.set_page_config(page_title="Consultant VÃ¢nzÄƒri IT AI", layout="wide")

# --- 1. GESTIONARE BAZÄ‚ DE DATE (SQLite) ---
DB_FILE = "chat_history.db"

def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS messages
                 (session_id TEXT, role TEXT, content TEXT, timestamp DATETIME)''')
    conn.commit()
    conn.close()

def save_message(session_id, role, content):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("INSERT INTO messages VALUES (?, ?, ?, ?)", 
              (session_id, role, content, datetime.now()))
    conn.commit()
    conn.close()

def load_history(session_id):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT role, content FROM messages WHERE session_id = ? ORDER BY timestamp", (session_id,))
    rows = c.fetchall()
    conn.close()
    # FormatÄƒm pentru Streamlit
    history = []
    for role, content in rows:
        history.append({"role": role, "content": content})
    return history

def clear_session_history(session_id):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("DELETE FROM messages WHERE session_id = ?", (session_id,))
    conn.commit()
    conn.close()

# IniÈ›ializÄƒm baza de date la pornire
init_db()

# --- 2. GESTIONARE ID SESIUNE (Query Params) ---
# VerificÄƒm dacÄƒ existÄƒ un ID Ã®n URL, altfel creÄƒm unul
if "session_id" not in st.query_params:
    new_id = str(uuid.uuid4())
    st.query_params["session_id"] = new_id
    session_id = new_id
else:
    session_id = st.query_params["session_id"]

# --- 3. GESTIONARE CHEI API (RotaÈ›ie & Fallback) ---
def configure_gemini():
    """
    ÃncearcÄƒ cheile din st.secrets. DacÄƒ una e expiratÄƒ, trece la urmÄƒtoarea.
    DacÄƒ nu existÄƒ chei valide, cere utilizatorului una.
    ReturneazÄƒ modelul configurat sau None.
    """
    api_keys = []
    
    # ÃncercÄƒm sÄƒ luÄƒm cheile din secrets (formatate ca listÄƒ sau string cu virgulÄƒ)
    if "api_keys" in st.secrets:
        if isinstance(st.secrets["api_keys"], list):
            api_keys = st.secrets["api_keys"]
        else:
            api_keys = st.secrets["api_keys"].split(",")
    
    valid_model = None
    working_key = None

    # IterÄƒm prin cheile definite Ã®n secrets
    for key in api_keys:
        key = key.strip()
        try:
            genai.configure(api_key=key)
            model = genai.GenerativeModel('gemini-1.5-flash')
            # Test rapid pentru a vedea dacÄƒ cheia e activÄƒ
            response = model.generate_content("test", request_options={"timeout": 5})
            working_key = key
            valid_model = model
            break # Am gÄƒsit o cheie bunÄƒ
        except Exception as e:
            st.sidebar.error(f"Cheia care se terminÄƒ Ã®n ...{key[-4:]} a expirat sau e invalidÄƒ.")
            continue

    # DacÄƒ nu am gÄƒsit nicio cheie validÄƒ Ã®n secrets, cerem Ã®n UI
    if not valid_model:
        st.sidebar.warning("Nicio cheie API din sistem nu funcÈ›ioneazÄƒ.")
        user_key = st.sidebar.text_input("Introdu o cheie API Google Gemini validÄƒ:", type="password")
        if user_key:
            try:
                genai.configure(api_key=user_key)
                model = genai.GenerativeModel('gemini-1.5-flash')
                model.generate_content("test")
                valid_model = model
                st.sidebar.success("Cheie utilizator validatÄƒ!")
            except Exception as e:
                st.sidebar.error("Cheia introdusÄƒ nu este validÄƒ.")
    
    return valid_model

# --- 4. FUNCÈšII UPLOAD FIÈ˜IERE ---
def upload_to_gemini(uploaded_file):
    """ÃncarcÄƒ fiÈ™ierul temporar È™i Ã®l trimite la Google Gemini"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name

        # Upload cÄƒtre Gemini
        gemini_file = genai.upload_file(path=tmp_path, display_name=uploaded_file.name)
        
        # AÈ™teptÄƒm procesarea (doar dacÄƒ e necesar, de obicei pt video/audio mari, dar bun ca practicÄƒ)
        while gemini_file.state.name == "PROCESSING":
            time.sleep(1)
            gemini_file = genai.get_file(gemini_file.name)
            
        os.remove(tmp_path) # È˜tergem local
        return gemini_file
    except Exception as e:
        st.error(f"Eroare la upload: {e}")
        return None

# --- INTERFAÈšA GRAFICÄ‚ (UI) ---

st.title("ğŸ¤– Asistent VÃ¢nzÄƒri IT - AI")
st.markdown(f"**ID Sesiune:** `{session_id}` (PoÈ›i reveni pe acest link pentru a continua discuÈ›ia)")

# Configurare Model
model = configure_gemini()

# Sidebar
with st.sidebar:
    st.header("ğŸ“‚ Documente Companie")
    st.info("ÃncarcÄƒ documentele pentru a oferi context AI-ului.")
    
    portfolio_file = st.file_uploader("Portofoliu Companie (PDF)", type=['pdf'])
    catalog_file = st.file_uploader("Catalog Produse & PreÈ›uri (PDF/TXT/CSV)", type=['pdf', 'txt', 'csv'])
    
    files_context = []
    
    if st.button("ProceseazÄƒ Documentele"):
        if model:
            with st.spinner("Se Ã®ncarcÄƒ fiÈ™ierele pe serverele Google..."):
                if portfolio_file:
                    f1 = upload_to_gemini(portfolio_file)
                    if f1: 
                        st.session_state['portfolio_ref'] = f1
                        st.success("Portofoliu Ã®ncÄƒrcat!")
                
                if catalog_file:
                    f2 = upload_to_gemini(catalog_file)
                    if f2: 
                        st.session_state['catalog_ref'] = f2
                        st.success("Catalog Ã®ncÄƒrcat!")
        else:
            st.error("Modelul AI nu este configurat. VerificÄƒ cheile API.")

    st.divider()
    if st.button("RESET CONVERSAÈšIE", type="primary"):
        clear_session_history(session_id)
        st.rerun()

# Recuperare istoric din SQLite
if "messages" not in st.session_state:
    st.session_state.messages = load_history(session_id)

# AfiÈ™are chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Zona de input
if prompt := st.chat_input("Ex: Clientul vrea o ofertÄƒ pentru 10 laptopuri È™i server de stocare..."):
    if not model:
        st.error("Te rog configureazÄƒ o cheie API validÄƒ Ã®n sidebar.")
    else:
        # 1. AdÄƒugÄƒm mesajul utilizatorului Ã®n UI È™i DB
        st.session_state.messages.append({"role": "user", "content": prompt})
        save_message(session_id, "user", prompt)
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. PregÄƒtim contextul pentru AI
        conversation_context = []
        
        # InstrucÈ›iuni de sistem
        system_instruction = """
        EÈ™ti un agent expert Ã®n vÃ¢nzÄƒri IT. 
        Rolul tÄƒu este sÄƒ analizezi cerinÈ›ele clientului È™i sÄƒ propui soluÈ›ii folosind DOAR echipamentele È™i serviciile din fiÈ™ierele Ã®ncÄƒrcate (dacÄƒ existÄƒ).
        DacÄƒ utilizatorul cere o ofertÄƒ, genereaz-o Ã®ntr-un format clar, tabelar, cu preÈ›uri (dacÄƒ sunt disponibile Ã®n catalog).
        Fii politicos, profesionist È™i orientat spre vÃ¢nzare.
        """
        
        # AdÄƒugÄƒm fiÈ™ierele Ã®ncÄƒrcate Ã®n request (dacÄƒ existÄƒ Ã®n sesiune)
        current_request = [system_instruction]
        
        if 'portfolio_ref' in st.session_state:
            current_request.append("Acesta este portofoliul companiei:")
            current_request.append(st.session_state['portfolio_ref'])
            
        if 'catalog_ref' in st.session_state:
            current_request.append("Acesta este catalogul de produse È™i preÈ›uri:")
            current_request.append(st.session_state['catalog_ref'])
            
        # AdÄƒugÄƒm istoricul conversaÈ›iei (pentru context conversaÈ›ional)
        # Nota: Gemini API 1.5 suportÄƒ istoric mare, dar aici simplificÄƒm trimiÈ›Ã¢nd promptul curent + fiÈ™ierele.
        # Pentru chat history complet cu fiÈ™iere, se foloseÈ™te start_chat, dar e complex cu fiÈ™ierele stateless.
        # O abordare hibridÄƒ: trimitem istoricul recent text + fiÈ™ierele la fiecare call (stateless approach).
        
        history_text = "\n".join([f"{m['role'].upper()}: {m['content']}" for m in st.session_state.messages[-5:]]) # Ultimele 5 mesaje context
        current_request.append(f"Istoric recent discuÈ›ie:\n{history_text}")
        current_request.append(f"SOLICITARE CURENTÄ‚: {prompt}")

        # 3. GenerÄƒm rÄƒspunsul
        with st.chat_message("assistant"):
            with st.spinner("AI-ul analizeazÄƒ cererea È™i portofoliul..."):
                try:
                    response = model.generate_content(current_request)
                    response_text = response.text
                    
                    st.markdown(response_text)
                    
                    # Salvare Ã®n DB È™i Sesiune
                    st.session_state.messages.append({"role": "assistant", "content": response_text})
                    save_message(session_id, "assistant", response_text)

                    # Buton descÄƒrcare ofertÄƒ
                    st.download_button(
                        label="ğŸ“¥ DescarcÄƒ RÄƒspunsul / Oferta (TXT)",
                        data=response_text,
                        file_name=f"oferta_{datetime.now().strftime('%Y%m%d_%H%M')}.txt",
                        mime="text/plain"
                    )

                except Exception as e:
                    st.error(f"A apÄƒrut o eroare la generare: {e}")
